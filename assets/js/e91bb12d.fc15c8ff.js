"use strict";(globalThis.webpackChunkmy_book=globalThis.webpackChunkmy_book||[]).push([[2205],{367(s,e,a){a.r(e),a.d(e,{assets:()=>m,contentTitle:()=>t,default:()=>d,frontMatter:()=>r,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"textbook/modules/learning-based-control/11","title":"Learning-Based Control","description":"Introduction","source":"@site/docs/textbook/modules/11-learning-based-control/theory.md","sourceDirName":"textbook/modules/11-learning-based-control","slug":"/textbook/modules/learning-based-control/learning-based-control","permalink":"/Hackathon-3/docs/textbook/modules/learning-based-control/learning-based-control","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/textbook/modules/11-learning-based-control/theory.md","tags":[],"version":"current","frontMatter":{"id":"11","title":"Learning-Based Control","slug":"learning-based-control","week":11,"difficulty":"advanced","prerequisites":["05","07"],"learning_objectives":["Implement reinforcement learning algorithms for continuous control tasks","Apply imitation learning from human demonstrations","Design reward functions that induce desired robot behavior","Combine learning with model-based control for sample efficiency"],"estimated_hours":16,"status":"draft","last_updated":"2025-01-01T00:00:00.000Z","version":"1.0.0"},"sidebar":"tutorialSidebar","previous":{"title":"Simulation to Real","permalink":"/Hackathon-3/docs/textbook/modules/simulation-to-real/simulation-to-real"},"next":{"title":"Human-Robot Interaction","permalink":"/Hackathon-3/docs/textbook/modules/human-robot-interaction/human-robot-interaction"}}');var i=a(4848),l=a(8453);const r={id:"11",title:"Learning-Based Control",slug:"learning-based-control",week:11,difficulty:"advanced",prerequisites:["05","07"],learning_objectives:["Implement reinforcement learning algorithms for continuous control tasks","Apply imitation learning from human demonstrations","Design reward functions that induce desired robot behavior","Combine learning with model-based control for sample efficiency"],estimated_hours:16,status:"draft",last_updated:new Date("2025-01-01T00:00:00.000Z"),version:"1.0.0"},t="Module 11: Learning-Based Control",m={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Section 1: Reinforcement Learning Foundations",id:"section-1-reinforcement-learning-foundations",level:2},{value:"1.1 Markov Decision Processes",id:"11-markov-decision-processes",level:3},{value:"1.2 Policy Gradient Methods",id:"12-policy-gradient-methods",level:3},{value:"Section 2: Deep RL Algorithms",id:"section-2-deep-rl-algorithms",level:2},{value:"2.1 PPO (Proximal Policy Optimization)",id:"21-ppo-proximal-policy-optimization",level:3},{value:"2.2 SAC (Soft Actor-Critic)",id:"22-sac-soft-actor-critic",level:3},{value:"Section 3: Imitation Learning",id:"section-3-imitation-learning",level:2},{value:"3.1 Behavioral Cloning",id:"31-behavioral-cloning",level:3},{value:"3.2 DAgger",id:"32-dagger",level:3},{value:"Section 4: Reward Engineering",id:"section-4-reward-engineering",level:2},{value:"4.1 Reward Design",id:"41-reward-design",level:3},{value:"4.2 Learning from Preferences",id:"42-learning-from-preferences",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Further Reading",id:"further-reading",level:2}];function h(s){const e={annotation:"annotation",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mover:"mover",mrow:"mrow",msub:"msub",msup:"msup",mtext:"mtext",ol:"ol",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,l.R)(),...s.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"module-11-learning-based-control",children:"Module 11: Learning-Based Control"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(e.p,{children:"Learning enables robots to acquire skills that are difficult to program explicitly. This module covers reinforcement learning, imitation learning, and their application to robotic control."}),"\n",(0,i.jsx)(e.h2,{id:"section-1-reinforcement-learning-foundations",children:"Section 1: Reinforcement Learning Foundations"}),"\n",(0,i.jsx)(e.h3,{id:"11-markov-decision-processes",children:"1.1 Markov Decision Processes"}),"\n",(0,i.jsx)("definition",{id:"def-mdp",children:(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Markov Decision Process (MDP)"}),": A mathematical framework for sequential decision-making defined by states, actions, transitions, and rewards."]})}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsxs)(e.span,{className:"katex",children:[(0,i.jsx)(e.span,{className:"katex-mathml",children:(0,i.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(e.semantics,{children:[(0,i.jsxs)(e.mrow,{children:[(0,i.jsxs)(e.msup,{children:[(0,i.jsx)(e.mi,{children:"V"}),(0,i.jsx)(e.mo,{children:"\u2217"})]}),(0,i.jsx)(e.mo,{stretchy:"false",children:"("}),(0,i.jsx)(e.mi,{children:"s"}),(0,i.jsx)(e.mo,{stretchy:"false",children:")"}),(0,i.jsx)(e.mo,{children:"="}),(0,i.jsxs)(e.msub,{children:[(0,i.jsxs)(e.mrow,{children:[(0,i.jsx)(e.mi,{children:"max"}),(0,i.jsx)(e.mo,{children:"\u2061"})]}),(0,i.jsx)(e.mi,{children:"a"})]}),(0,i.jsxs)(e.mrow,{children:[(0,i.jsx)(e.mo,{fence:"true",children:"["}),(0,i.jsx)(e.mi,{children:"R"}),(0,i.jsx)(e.mo,{stretchy:"false",children:"("}),(0,i.jsx)(e.mi,{children:"s"}),(0,i.jsx)(e.mo,{separator:"true",children:","}),(0,i.jsx)(e.mi,{children:"a"}),(0,i.jsx)(e.mo,{stretchy:"false",children:")"}),(0,i.jsx)(e.mo,{children:"+"}),(0,i.jsx)(e.mi,{children:"\u03b3"}),(0,i.jsxs)(e.msub,{children:[(0,i.jsx)(e.mo,{children:"\u2211"}),(0,i.jsxs)(e.msup,{children:[(0,i.jsx)(e.mi,{children:"s"}),(0,i.jsx)(e.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]})]}),(0,i.jsx)(e.mi,{children:"P"}),(0,i.jsx)(e.mo,{stretchy:"false",children:"("}),(0,i.jsxs)(e.msup,{children:[(0,i.jsx)(e.mi,{children:"s"}),(0,i.jsx)(e.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(e.mi,{mathvariant:"normal",children:"\u2223"}),(0,i.jsx)(e.mi,{children:"s"}),(0,i.jsx)(e.mo,{separator:"true",children:","}),(0,i.jsx)(e.mi,{children:"a"}),(0,i.jsx)(e.mo,{stretchy:"false",children:")"}),(0,i.jsxs)(e.msup,{children:[(0,i.jsx)(e.mi,{children:"V"}),(0,i.jsx)(e.mo,{children:"\u2217"})]}),(0,i.jsx)(e.mo,{stretchy:"false",children:"("}),(0,i.jsxs)(e.msup,{children:[(0,i.jsx)(e.mi,{children:"s"}),(0,i.jsx)(e.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(e.mo,{stretchy:"false",children:")"}),(0,i.jsx)(e.mo,{fence:"true",children:"]"})]})]}),(0,i.jsx)(e.annotation,{encoding:"application/x-tex",children:"V^*(s) = \\max_a \\left[R(s,a) + \\gamma \\sum_{s'} P(s'|s,a)V^*(s')\\right]"})]})})}),(0,i.jsxs)(e.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(e.span,{className:"base",children:[(0,i.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsxs)(e.span,{className:"mord",children:[(0,i.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.22222em"},children:"V"}),(0,i.jsx)(e.span,{className:"msupsub",children:(0,i.jsx)(e.span,{className:"vlist-t",children:(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.6887em"},children:(0,i.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(e.span,{className:"mbin mtight",children:"\u2217"})})]})})})})})]}),(0,i.jsx)(e.span,{className:"mopen",children:"("}),(0,i.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(e.span,{className:"mclose",children:")"}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(e.span,{className:"mrel",children:"="}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(e.span,{className:"base",children:[(0,i.jsx)(e.span,{className:"strut",style:{height:"1.0516em",verticalAlign:"-0.2997em"}}),(0,i.jsxs)(e.span,{className:"mop",children:[(0,i.jsx)(e.span,{className:"mop",children:"max"}),(0,i.jsx)(e.span,{className:"msupsub",children:(0,i.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(e.span,{className:"vlist-r",children:[(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.1514em"},children:(0,i.jsxs)(e.span,{style:{top:"-2.55em",marginRight:"0.05em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(e.span,{className:"mord mathnormal mtight",children:"a"})})]})}),(0,i.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(e.span,{})})})]})})]}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(e.span,{className:"minner",children:[(0,i.jsx)(e.span,{className:"mopen delimcenter",style:{top:"0em"},children:"["}),(0,i.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.00773em"},children:"R"}),(0,i.jsx)(e.span,{className:"mopen",children:"("}),(0,i.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(e.span,{className:"mpunct",children:","}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(e.span,{className:"mclose",children:")"}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(e.span,{className:"mbin",children:"+"}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.05556em"},children:"\u03b3"}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(e.span,{className:"mop",children:[(0,i.jsx)(e.span,{className:"mop op-symbol small-op",style:{position:"relative",top:"0em"},children:"\u2211"}),(0,i.jsx)(e.span,{className:"msupsub",children:(0,i.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(e.span,{className:"vlist-r",children:[(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.1783em"},children:(0,i.jsxs)(e.span,{style:{top:"-2.4003em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(e.span,{className:"mord mtight",children:(0,i.jsxs)(e.span,{className:"mord mtight",children:[(0,i.jsx)(e.span,{className:"mord mathnormal mtight",children:"s"}),(0,i.jsx)(e.span,{className:"msupsub",children:(0,i.jsx)(e.span,{className:"vlist-t",children:(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.6828em"},children:(0,i.jsxs)(e.span,{style:{top:"-2.786em",marginRight:"0.0714em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"2.5em"}}),(0,i.jsx)(e.span,{className:"sizing reset-size3 size1 mtight",children:(0,i.jsx)(e.span,{className:"mord mtight",children:(0,i.jsx)(e.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]})})})]})}),(0,i.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.2997em"},children:(0,i.jsx)(e.span,{})})})]})})]}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"P"}),(0,i.jsx)(e.span,{className:"mopen",children:"("}),(0,i.jsxs)(e.span,{className:"mord",children:[(0,i.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(e.span,{className:"msupsub",children:(0,i.jsx)(e.span,{className:"vlist-t",children:(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.7519em"},children:(0,i.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(e.span,{className:"mord mtight",children:(0,i.jsx)(e.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(e.span,{className:"mord",children:"\u2223"}),(0,i.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(e.span,{className:"mpunct",children:","}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(e.span,{className:"mclose",children:")"}),(0,i.jsxs)(e.span,{className:"mord",children:[(0,i.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.22222em"},children:"V"}),(0,i.jsx)(e.span,{className:"msupsub",children:(0,i.jsx)(e.span,{className:"vlist-t",children:(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.6887em"},children:(0,i.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(e.span,{className:"mbin mtight",children:"\u2217"})})]})})})})})]}),(0,i.jsx)(e.span,{className:"mopen",children:"("}),(0,i.jsxs)(e.span,{className:"mord",children:[(0,i.jsx)(e.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(e.span,{className:"msupsub",children:(0,i.jsx)(e.span,{className:"vlist-t",children:(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.7519em"},children:(0,i.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(e.span,{className:"mord mtight",children:(0,i.jsx)(e.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(e.span,{className:"mclose",children:")"}),(0,i.jsx)(e.span,{className:"mclose delimcenter",style:{top:"0em"},children:"]"})]})]})]})]})}),"\n",(0,i.jsx)(e.h3,{id:"12-policy-gradient-methods",children:"1.2 Policy Gradient Methods"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"def policy_gradient_update(policy, trajectories):\n    loss = 0\n    for traj in trajectories:\n        returns = compute_returns(traj.rewards)\n        for t, (s, a, r, R) in enumerate(zip(...)):\n            log_prob = policy.log_prob(s, a)\n            loss -= log_prob * R  # REINFORCE\n    loss.backward()\n    optimizer.step()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"section-2-deep-rl-algorithms",children:"Section 2: Deep RL Algorithms"}),"\n",(0,i.jsx)(e.h3,{id:"21-ppo-proximal-policy-optimization",children:"2.1 PPO (Proximal Policy Optimization)"}),"\n",(0,i.jsx)(e.p,{children:"Stable policy updates through clipping:"}),"\n",(0,i.jsx)("equation",{id:"eq-ppo",children:(0,i.jsx)(e.p,{children:(0,i.jsxs)(e.span,{className:"katex",children:[(0,i.jsx)(e.span,{className:"katex-mathml",children:(0,i.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(e.semantics,{children:[(0,i.jsxs)(e.mrow,{children:[(0,i.jsxs)(e.msup,{children:[(0,i.jsx)(e.mi,{children:"L"}),(0,i.jsxs)(e.mrow,{children:[(0,i.jsx)(e.mi,{children:"C"}),(0,i.jsx)(e.mi,{children:"L"}),(0,i.jsx)(e.mi,{children:"I"}),(0,i.jsx)(e.mi,{children:"P"})]})]}),(0,i.jsx)(e.mo,{stretchy:"false",children:"("}),(0,i.jsx)(e.mi,{children:"\u03b8"}),(0,i.jsx)(e.mo,{stretchy:"false",children:")"}),(0,i.jsx)(e.mo,{children:"="}),(0,i.jsx)(e.mi,{mathvariant:"double-struck",children:"E"}),(0,i.jsx)(e.mo,{stretchy:"false",children:"["}),(0,i.jsx)(e.mi,{children:"min"}),(0,i.jsx)(e.mo,{children:"\u2061"}),(0,i.jsx)(e.mo,{stretchy:"false",children:"("}),(0,i.jsxs)(e.msub,{children:[(0,i.jsx)(e.mi,{children:"r"}),(0,i.jsx)(e.mi,{children:"t"})]}),(0,i.jsx)(e.mo,{stretchy:"false",children:"("}),(0,i.jsx)(e.mi,{children:"\u03b8"}),(0,i.jsx)(e.mo,{stretchy:"false",children:")"}),(0,i.jsxs)(e.msub,{children:[(0,i.jsxs)(e.mover,{accent:"true",children:[(0,i.jsx)(e.mi,{children:"A"}),(0,i.jsx)(e.mo,{children:"^"})]}),(0,i.jsx)(e.mi,{children:"t"})]}),(0,i.jsx)(e.mo,{separator:"true",children:","}),(0,i.jsx)(e.mtext,{children:"clip"}),(0,i.jsx)(e.mo,{stretchy:"false",children:"("}),(0,i.jsxs)(e.msub,{children:[(0,i.jsx)(e.mi,{children:"r"}),(0,i.jsx)(e.mi,{children:"t"})]}),(0,i.jsx)(e.mo,{stretchy:"false",children:"("}),(0,i.jsx)(e.mi,{children:"\u03b8"}),(0,i.jsx)(e.mo,{stretchy:"false",children:")"}),(0,i.jsx)(e.mo,{separator:"true",children:","}),(0,i.jsx)(e.mn,{children:"1"}),(0,i.jsx)(e.mo,{children:"\u2212"}),(0,i.jsx)(e.mi,{children:"\u03f5"}),(0,i.jsx)(e.mo,{separator:"true",children:","}),(0,i.jsx)(e.mn,{children:"1"}),(0,i.jsx)(e.mo,{children:"+"}),(0,i.jsx)(e.mi,{children:"\u03f5"}),(0,i.jsx)(e.mo,{stretchy:"false",children:")"}),(0,i.jsxs)(e.msub,{children:[(0,i.jsxs)(e.mover,{accent:"true",children:[(0,i.jsx)(e.mi,{children:"A"}),(0,i.jsx)(e.mo,{children:"^"})]}),(0,i.jsx)(e.mi,{children:"t"})]}),(0,i.jsx)(e.mo,{stretchy:"false",children:")"}),(0,i.jsx)(e.mo,{stretchy:"false",children:"]"})]}),(0,i.jsx)(e.annotation,{encoding:"application/x-tex",children:"L^{CLIP}(\\theta) = \\mathbb{E}[\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]"})]})})}),(0,i.jsxs)(e.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(e.span,{className:"base",children:[(0,i.jsx)(e.span,{className:"strut",style:{height:"1.0913em",verticalAlign:"-0.25em"}}),(0,i.jsxs)(e.span,{className:"mord",children:[(0,i.jsx)(e.span,{className:"mord mathnormal",children:"L"}),(0,i.jsx)(e.span,{className:"msupsub",children:(0,i.jsx)(e.span,{className:"vlist-t",children:(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.8413em"},children:(0,i.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsxs)(e.span,{className:"mord mtight",children:[(0,i.jsx)(e.span,{className:"mord mathnormal mtight",style:{marginRight:"0.07153em"},children:"C"}),(0,i.jsx)(e.span,{className:"mord mathnormal mtight",children:"L"}),(0,i.jsx)(e.span,{className:"mord mathnormal mtight",style:{marginRight:"0.07847em"},children:"I"}),(0,i.jsx)(e.span,{className:"mord mathnormal mtight",style:{marginRight:"0.13889em"},children:"P"})]})})]})})})})})]}),(0,i.jsx)(e.span,{className:"mopen",children:"("}),(0,i.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"\u03b8"}),(0,i.jsx)(e.span,{className:"mclose",children:")"}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(e.span,{className:"mrel",children:"="}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(e.span,{className:"base",children:[(0,i.jsx)(e.span,{className:"strut",style:{height:"1.1968em",verticalAlign:"-0.25em"}}),(0,i.jsx)(e.span,{className:"mord mathbb",children:"E"}),(0,i.jsx)(e.span,{className:"mopen",children:"["}),(0,i.jsx)(e.span,{className:"mop",children:"min"}),(0,i.jsx)(e.span,{className:"mopen",children:"("}),(0,i.jsxs)(e.span,{className:"mord",children:[(0,i.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"r"}),(0,i.jsx)(e.span,{className:"msupsub",children:(0,i.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(e.span,{className:"vlist-r",children:[(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.2806em"},children:(0,i.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"-0.0278em",marginRight:"0.05em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(e.span,{className:"mord mathnormal mtight",children:"t"})})]})}),(0,i.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(e.span,{})})})]})})]}),(0,i.jsx)(e.span,{className:"mopen",children:"("}),(0,i.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"\u03b8"}),(0,i.jsx)(e.span,{className:"mclose",children:")"}),(0,i.jsxs)(e.span,{className:"mord",children:[(0,i.jsx)(e.span,{className:"mord accent",children:(0,i.jsx)(e.span,{className:"vlist-t",children:(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsxs)(e.span,{className:"vlist",style:{height:"0.9468em"},children:[(0,i.jsxs)(e.span,{style:{top:"-3em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,i.jsx)(e.span,{className:"mord mathnormal",children:"A"})]}),(0,i.jsxs)(e.span,{style:{top:"-3.2523em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,i.jsx)(e.span,{className:"accent-body",style:{left:"-0.1111em"},children:(0,i.jsx)(e.span,{className:"mord",children:"^"})})]})]})})})}),(0,i.jsx)(e.span,{className:"msupsub",children:(0,i.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(e.span,{className:"vlist-r",children:[(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.2806em"},children:(0,i.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(e.span,{className:"mord mathnormal mtight",children:"t"})})]})}),(0,i.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(e.span,{})})})]})})]}),(0,i.jsx)(e.span,{className:"mpunct",children:","}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(e.span,{className:"mord text",children:(0,i.jsx)(e.span,{className:"mord",children:"clip"})}),(0,i.jsx)(e.span,{className:"mopen",children:"("}),(0,i.jsxs)(e.span,{className:"mord",children:[(0,i.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"r"}),(0,i.jsx)(e.span,{className:"msupsub",children:(0,i.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(e.span,{className:"vlist-r",children:[(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.2806em"},children:(0,i.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"-0.0278em",marginRight:"0.05em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(e.span,{className:"mord mathnormal mtight",children:"t"})})]})}),(0,i.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(e.span,{})})})]})})]}),(0,i.jsx)(e.span,{className:"mopen",children:"("}),(0,i.jsx)(e.span,{className:"mord mathnormal",style:{marginRight:"0.02778em"},children:"\u03b8"}),(0,i.jsx)(e.span,{className:"mclose",children:")"}),(0,i.jsx)(e.span,{className:"mpunct",children:","}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(e.span,{className:"mord",children:"1"}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(e.span,{className:"mbin",children:"\u2212"}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(e.span,{className:"base",children:[(0,i.jsx)(e.span,{className:"strut",style:{height:"0.8389em",verticalAlign:"-0.1944em"}}),(0,i.jsx)(e.span,{className:"mord mathnormal",children:"\u03f5"}),(0,i.jsx)(e.span,{className:"mpunct",children:","}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(e.span,{className:"mord",children:"1"}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(e.span,{className:"mbin",children:"+"}),(0,i.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(e.span,{className:"base",children:[(0,i.jsx)(e.span,{className:"strut",style:{height:"1.1968em",verticalAlign:"-0.25em"}}),(0,i.jsx)(e.span,{className:"mord mathnormal",children:"\u03f5"}),(0,i.jsx)(e.span,{className:"mclose",children:")"}),(0,i.jsxs)(e.span,{className:"mord",children:[(0,i.jsx)(e.span,{className:"mord accent",children:(0,i.jsx)(e.span,{className:"vlist-t",children:(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsxs)(e.span,{className:"vlist",style:{height:"0.9468em"},children:[(0,i.jsxs)(e.span,{style:{top:"-3em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,i.jsx)(e.span,{className:"mord mathnormal",children:"A"})]}),(0,i.jsxs)(e.span,{style:{top:"-3.2523em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,i.jsx)(e.span,{className:"accent-body",style:{left:"-0.1111em"},children:(0,i.jsx)(e.span,{className:"mord",children:"^"})})]})]})})})}),(0,i.jsx)(e.span,{className:"msupsub",children:(0,i.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(e.span,{className:"vlist-r",children:[(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.2806em"},children:(0,i.jsxs)(e.span,{style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"},children:[(0,i.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(e.span,{className:"mord mathnormal mtight",children:"t"})})]})}),(0,i.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(e.span,{className:"vlist-r",children:(0,i.jsx)(e.span,{className:"vlist",style:{height:"0.15em"},children:(0,i.jsx)(e.span,{})})})]})})]}),(0,i.jsx)(e.span,{className:"mclose",children:")]"})]})]})]})})}),"\n",(0,i.jsx)(e.h3,{id:"22-sac-soft-actor-critic",children:"2.2 SAC (Soft Actor-Critic)"}),"\n",(0,i.jsx)(e.p,{children:"Maximum entropy RL for exploration."}),"\n",(0,i.jsx)("warning",{children:(0,i.jsx)(e.p,{children:"RL requires many samples. Simulation training is typically necessary before real-world deployment."})}),"\n",(0,i.jsx)(e.h2,{id:"section-3-imitation-learning",children:"Section 3: Imitation Learning"}),"\n",(0,i.jsx)(e.h3,{id:"31-behavioral-cloning",children:"3.1 Behavioral Cloning"}),"\n",(0,i.jsx)(e.p,{children:"Supervised learning from demonstrations:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"def behavioral_cloning(demonstrations):\n    model = PolicyNetwork()\n    for epoch in range(epochs):\n        for state, action in demonstrations:\n            pred_action = model(state)\n            loss = mse_loss(pred_action, action)\n            loss.backward()\n            optimizer.step()\n    return model\n"})}),"\n",(0,i.jsx)(e.h3,{id:"32-dagger",children:"3.2 DAgger"}),"\n",(0,i.jsx)(e.p,{children:"Dataset Aggregation for correcting distribution shift."}),"\n",(0,i.jsx)(e.h2,{id:"section-4-reward-engineering",children:"Section 4: Reward Engineering"}),"\n",(0,i.jsx)(e.h3,{id:"41-reward-design",children:"4.1 Reward Design"}),"\n",(0,i.jsx)(e.p,{children:"Challenges:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Sparse rewards (hard to learn)"}),"\n",(0,i.jsx)(e.li,{children:"Dense rewards (reward hacking)"}),"\n",(0,i.jsx)(e.li,{children:"Multi-objective tradeoffs"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"42-learning-from-preferences",children:"4.2 Learning from Preferences"}),"\n",(0,i.jsx)(e.p,{children:"Using human feedback to shape rewards."}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"Key takeaways:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"RL enables skill acquisition through trial and error"}),"\n",(0,i.jsx)(e.li,{children:"PPO and SAC are practical algorithms for robotics"}),"\n",(0,i.jsx)(e.li,{children:"Imitation learning leverages human expertise"}),"\n",(0,i.jsx)(e.li,{children:"Reward design significantly impacts learning outcomes"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Policy Gradient"}),": Learning by gradient ascent on expected return"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Actor-Critic"}),": Combining policy and value function learning"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Imitation Learning"}),": Learning from demonstrations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Reward Shaping"}),": Designing rewards for desired behavior"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:'Sutton, R.S. & Barto, A.G. (2018). "Reinforcement Learning: An Introduction"'}),"\n",(0,i.jsx)(e.li,{children:'Levine, S. et al. (2016). "End-to-End Training of Deep Visuomotor Policies"'}),"\n"]})]})}function d(s={}){const{wrapper:e}={...(0,l.R)(),...s.components};return e?(0,i.jsx)(e,{...s,children:(0,i.jsx)(h,{...s})}):h(s)}},8453(s,e,a){a.d(e,{R:()=>r,x:()=>t});var n=a(6540);const i={},l=n.createContext(i);function r(s){const e=n.useContext(l);return n.useMemo(function(){return"function"==typeof s?s(e):{...e,...s}},[e,s])}function t(s){let e;return e=s.disableParentContext?"function"==typeof s.components?s.components(i):s.components||i:r(s.components),n.createElement(l.Provider,{value:e},s.children)}}}]);