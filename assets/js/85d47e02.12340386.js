"use strict";(globalThis.webpackChunkmy_book=globalThis.webpackChunkmy_book||[]).push([[7556],{8453(e,n,r){r.d(n,{R:()=>o,x:()=>d});var i=r(6540);const t={},s=i.createContext(t);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:n},e.children)}},9244(e,n,r){r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>d,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"textbook/modules/sensors-perception/labs/lab-04-02","title":"Camera Image Processing for Robotics","description":"Objectives","source":"@site/docs/textbook/modules/04-sensors-perception/labs/lab-04-02.md","sourceDirName":"textbook/modules/04-sensors-perception/labs","slug":"/textbook/modules/sensors-perception/labs/lab-04-02","permalink":"/Hackathon-3/docs/textbook/modules/sensors-perception/labs/lab-04-02","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/textbook/modules/04-sensors-perception/labs/lab-04-02.md","tags":[],"version":"current","frontMatter":{"id":"lab-04-02","module_id":"04","title":"Camera Image Processing for Robotics","difficulty":"intermediate","tier":"simulation","duration_minutes":90},"sidebar":"tutorialSidebar","previous":{"title":"Reading IMU Data in MuJoCo","permalink":"/Hackathon-3/docs/textbook/modules/sensors-perception/labs/lab-04-01"},"next":{"title":"Sensor Fusion with Extended Kalman Filter","permalink":"/Hackathon-3/docs/textbook/modules/sensors-perception/labs/lab-04-03"}}');var t=r(4848),s=r(8453);const o={id:"lab-04-02",module_id:"04",title:"Camera Image Processing for Robotics",difficulty:"intermediate",tier:"simulation",duration_minutes:90},d="Lab 04-02: Camera Image Processing for Robotics",a={},c=[{value:"Objectives",id:"objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Materials",id:"materials",level:2},{value:"Background",id:"background",level:2},{value:"Instructions",id:"instructions",level:2},{value:"Step 1: Camera Setup and Rendering",id:"step-1-camera-setup-and-rendering",level:3},{value:"Step 2: Camera Intrinsics",id:"step-2-camera-intrinsics",level:3},{value:"Step 3: Depth Image Processing",id:"step-3-depth-image-processing",level:3},{value:"Step 4: Color-Based Object Segmentation",id:"step-4-color-based-object-segmentation",level:3},{value:"Step 5: Depth-Based Point Cloud Generation",id:"step-5-depth-based-point-cloud-generation",level:3},{value:"Step 6: Point Cloud Visualization (Optional)",id:"step-6-point-cloud-visualization-optional",level:3},{value:"Expected Outcomes",id:"expected-outcomes",level:2},{value:"Rubric",id:"rubric",level:2},{value:"Common Errors",id:"common-errors",level:2},{value:"Extensions",id:"extensions",level:2},{value:"Related Content",id:"related-content",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lab-04-02-camera-image-processing-for-robotics",children:"Lab 04-02: Camera Image Processing for Robotics"})}),"\n",(0,t.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this lab, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Render camera images from MuJoCo simulation"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Process RGB and depth images for robotic perception"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement basic object detection using color segmentation"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Extract 3D point clouds from depth images"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completed Lab 04-01 (IMU data reading)"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of camera projection models"}),"\n",(0,t.jsx)(n.li,{children:"Familiarity with NumPy array operations"}),"\n",(0,t.jsx)(n.li,{children:"Basic OpenCV knowledge (helpful but not required)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"materials",children:"Materials"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Type"}),(0,t.jsx)(n.th,{children:"Name"}),(0,t.jsx)(n.th,{children:"Tier"}),(0,t.jsx)(n.th,{children:"Notes"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"software"}),(0,t.jsx)(n.td,{children:"MuJoCo 3.0+"}),(0,t.jsx)(n.td,{children:"required"}),(0,t.jsx)(n.td,{children:"Physics simulation with rendering"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"software"}),(0,t.jsx)(n.td,{children:"Python 3.10+"}),(0,t.jsx)(n.td,{children:"required"}),(0,t.jsx)(n.td,{children:"Programming environment"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"software"}),(0,t.jsx)(n.td,{children:"NumPy, OpenCV"}),(0,t.jsx)(n.td,{children:"required"}),(0,t.jsx)(n.td,{children:"Image processing"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"software"}),(0,t.jsx)(n.td,{children:"Open3D"}),(0,t.jsx)(n.td,{children:"recommended"}),(0,t.jsx)(n.td,{children:"Point cloud visualization"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"simulation"}),(0,t.jsx)(n.td,{children:"humanoid-camera.xml"}),(0,t.jsx)(n.td,{children:"required"}),(0,t.jsx)(n.td,{children:"Model with camera sensors"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"background",children:"Background"}),"\n",(0,t.jsx)(n.p,{children:"Cameras are essential sensors for robot perception, providing rich visual information about the environment. In robotics, we commonly use:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"RGB Cameras"}),": Color images for object recognition, tracking"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Depth Cameras"}),": Distance measurements for 3D reconstruction"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stereo Cameras"}),": Two cameras for depth estimation"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"MuJoCo supports rendering from arbitrary camera viewpoints, providing both color and depth buffers."}),"\n",(0,t.jsx)(n.h2,{id:"instructions",children:"Instructions"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-camera-setup-and-rendering",children:"Step 1: Camera Setup and Rendering"}),"\n",(0,t.jsx)(n.p,{children:"Configure and render from a robot-mounted camera:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import mujoco\nimport numpy as np\nimport cv2\n\n# Load model with camera\nmodel = mujoco.MjModel.from_xml_path("textbook/assets/robot-models/mujoco/humanoid-camera.xml")\ndata = mujoco.MjData(model)\n\n# Camera parameters\ncamera_name = "head_camera"\nwidth, height = 640, 480\n\n# Create renderer\nrenderer = mujoco.Renderer(model, height, width)\n\ndef render_camera(model, data, renderer, camera_name):\n    """\n    Render RGB and depth images from a named camera.\n\n    Returns:\n        Tuple of (rgb_image, depth_image)\n    """\n    # Update scene\n    renderer.update_scene(data, camera=camera_name)\n\n    # Render RGB\n    rgb = renderer.render()\n\n    # Render depth\n    renderer.enable_depth_rendering(True)\n    depth = renderer.render()\n    renderer.enable_depth_rendering(False)\n\n    return rgb, depth\n\n# Step simulation and render\nmujoco.mj_step(model, data)\nrgb_image, depth_image = render_camera(model, data, renderer, camera_name)\n\nprint(f"RGB shape: {rgb_image.shape}")\nprint(f"Depth shape: {depth_image.shape}")\nprint(f"Depth range: [{depth_image.min():.3f}, {depth_image.max():.3f}]")\n'})}),"\n",(0,t.jsx)("checkpoint",{children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected"}),": RGB image shape (480, 640, 3), depth image shape (480, 640). Depth values represent distance in meters."]})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-camera-intrinsics",children:"Step 2: Camera Intrinsics"}),"\n",(0,t.jsx)(n.p,{children:"Extract camera parameters for projection/unprojection:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def get_camera_intrinsics(model, camera_name, width, height):\n    """\n    Compute camera intrinsic matrix from MuJoCo camera parameters.\n\n    Returns:\n        K: 3x3 intrinsic matrix\n        fov_y: Vertical field of view in radians\n    """\n    camera_id = mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_CAMERA, camera_name)\n\n    # Get field of view (vertical)\n    fov_y = model.cam_fovy[camera_id] * np.pi / 180.0\n\n    # Compute focal lengths\n    fy = height / (2.0 * np.tan(fov_y / 2.0))\n    fx = fy  # Assuming square pixels\n\n    # Principal point (image center)\n    cx = width / 2.0\n    cy = height / 2.0\n\n    # Intrinsic matrix\n    K = np.array([\n        [fx, 0, cx],\n        [0, fy, cy],\n        [0, 0, 1]\n    ])\n\n    return K, fov_y\n\nK, fov_y = get_camera_intrinsics(model, camera_name, width, height)\nprint(f"Camera intrinsic matrix K:\\n{K}")\nprint(f"Vertical FOV: {np.degrees(fov_y):.1f} degrees")\n'})}),"\n",(0,t.jsx)("checkpoint",{children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected"}),": Valid intrinsic matrix with focal lengths around 500-800 pixels for typical FOV."]})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-depth-image-processing",children:"Step 3: Depth Image Processing"}),"\n",(0,t.jsx)(n.p,{children:"Convert depth buffer to metric depth and visualize:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def process_depth(depth_raw, near=0.01, far=10.0):\n    """\n    Process MuJoCo depth buffer to metric depth.\n\n    MuJoCo returns normalized depth in [0, 1] where:\n    - 0 = near plane\n    - 1 = far plane (or infinity for distant objects)\n\n    Args:\n        depth_raw: Raw depth buffer from renderer\n        near: Near clipping plane distance\n        far: Far clipping plane distance\n\n    Returns:\n        Metric depth image in meters\n    """\n    # Handle edge cases\n    depth_raw = np.clip(depth_raw, 0, 1)\n\n    # Convert to linear depth\n    # z_ndc = 2 * depth_raw - 1\n    # z_linear = 2 * near * far / (far + near - z_ndc * (far - near))\n\n    # Simplified for MuJoCo\'s depth convention\n    depth_metric = near + depth_raw * (far - near)\n\n    return depth_metric\n\ndef visualize_depth(depth_metric, max_depth=5.0):\n    """Create colorized depth visualization."""\n    # Normalize for visualization\n    depth_viz = np.clip(depth_metric / max_depth, 0, 1)\n    depth_viz = (depth_viz * 255).astype(np.uint8)\n\n    # Apply colormap\n    depth_colored = cv2.applyColorMap(depth_viz, cv2.COLORMAP_JET)\n\n    return depth_colored\n\n# Process and visualize depth\ndepth_metric = process_depth(depth_image)\ndepth_viz = visualize_depth(depth_metric)\n\n# Save images\ncv2.imwrite(\'rgb_view.png\', cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))\ncv2.imwrite(\'depth_view.png\', depth_viz)\n\nprint(f"Metric depth range: [{depth_metric.min():.3f}, {depth_metric.max():.3f}] meters")\n'})}),"\n",(0,t.jsx)("checkpoint",{children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected"}),": Depth visualization showing objects colored by distance (blue=close, red=far)."]})}),"\n",(0,t.jsx)(n.h3,{id:"step-4-color-based-object-segmentation",children:"Step 4: Color-Based Object Segmentation"}),"\n",(0,t.jsx)(n.p,{children:"Implement simple object detection using color:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def segment_by_color(rgb_image, target_hsv_range):\n    """\n    Segment objects by HSV color range.\n\n    Args:\n        rgb_image: Input RGB image\n        target_hsv_range: Tuple of ((h_min, s_min, v_min), (h_max, s_max, v_max))\n\n    Returns:\n        Binary mask where target color is detected\n    """\n    # Convert to HSV\n    hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)\n\n    # Create mask\n    lower = np.array(target_hsv_range[0])\n    upper = np.array(target_hsv_range[1])\n    mask = cv2.inRange(hsv, lower, upper)\n\n    # Clean up with morphological operations\n    kernel = np.ones((5, 5), np.uint8)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n    return mask\n\ndef find_object_centroid(mask):\n    """Find centroid of segmented object."""\n    # Find contours\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    if not contours:\n        return None\n\n    # Find largest contour\n    largest = max(contours, key=cv2.contourArea)\n\n    # Compute centroid\n    M = cv2.moments(largest)\n```python\n```python\n    if M["m00"] == 0:\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'        return None\n\n    cx = int(M["m10"] / M["m00"])\n    cy = int(M["m01"] / M["m00"])\n\n    return (cx, cy)\n\n# Example: Detect red objects\n# HSV range for red (note: red wraps around in HSV)\nred_range = ((0, 100, 100), (10, 255, 255))  # Lower red\nred_mask = segment_by_color(rgb_image, red_range)\n\n# Find centroid\ncentroid = find_object_centroid(red_mask)\nif centroid:\n    print(f"Object centroid in image: {centroid}")\n\n    # Draw on image\n    result = rgb_image.copy()\n    cv2.circle(result, centroid, 10, (0, 255, 0), 2)\n    cv2.imwrite(\'detection_result.png\', cv2.cvtColor(result, cv2.COLOR_RGB2BGR))\n'})}),"\n",(0,t.jsx)("checkpoint",{children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected"}),": If red objects are in the scene, the centroid should be detected and visualized."]})}),"\n",(0,t.jsx)(n.h3,{id:"step-5-depth-based-point-cloud-generation",children:"Step 5: Depth-Based Point Cloud Generation"}),"\n",(0,t.jsx)(n.p,{children:"Convert depth image to 3D point cloud:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def depth_to_pointcloud(depth_metric, K, rgb_image=None):\n    """\n    Convert depth image to 3D point cloud.\n\n    Args:\n        depth_metric: Depth image in meters\n        K: Camera intrinsic matrix\n        rgb_image: Optional RGB image for coloring points\n\n    Returns:\n        points: Nx3 array of 3D points\n        colors: Nx3 array of RGB colors (if rgb_image provided)\n    """\n    height, width = depth_metric.shape\n\n    # Create pixel coordinate grids\n    u = np.arange(width)\n    v = np.arange(height)\n    u, v = np.meshgrid(u, v)\n\n    # Unproject to 3D\n    fx, fy = K[0, 0], K[1, 1]\n    cx, cy = K[0, 2], K[1, 2]\n\n    z = depth_metric\n    x = (u - cx) * z / fx\n    y = (v - cy) * z / fy\n\n    # Stack into point cloud\n    points = np.stack([x, y, z], axis=-1)\n    points = points.reshape(-1, 3)\n\n    # Filter invalid points (too far or too close)\n```python\n```python\n    valid = (z.flatten() > 0.01) & (z.flatten() < 10.0)\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'    points = points[valid]\n\n    colors = None\n    if rgb_image is not None:\n        colors = rgb_image.reshape(-1, 3)[valid] / 255.0\n\n    return points, colors\n\n# Generate point cloud\npoints, colors = depth_to_pointcloud(depth_metric, K, rgb_image)\nprint(f"Point cloud: {points.shape[0]} points")\nprint(f"Bounding box: X[{points[:, 0].min():.2f}, {points[:, 0].max():.2f}]")\nprint(f"             Y[{points[:, 1].min():.2f}, {points[:, 1].max():.2f}]")\nprint(f"             Z[{points[:, 2].min():.2f}, {points[:, 2].max():.2f}]")\n'})}),"\n",(0,t.jsx)("checkpoint",{children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected"}),": Point cloud with thousands of valid 3D points forming the visible scene geometry."]})}),"\n",(0,t.jsx)(n.h3,{id:"step-6-point-cloud-visualization-optional",children:"Step 6: Point Cloud Visualization (Optional)"}),"\n",(0,t.jsx)(n.p,{children:"If Open3D is available, visualize the point cloud:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'try:\n    import open3d as o3d\n\n    def visualize_pointcloud(points, colors=None):\n        """Visualize point cloud using Open3D."""\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(points)\n\n        if colors is not None:\n            pcd.colors = o3d.utility.Vector3dVector(colors)\n\n        # Add coordinate frame\n        coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5)\n\n        # Visualize\n        o3d.visualization.draw_geometries([pcd, coord_frame])\n\n        # Save\n        o3d.io.write_point_cloud("scene_pointcloud.ply", pcd)\n        print("Saved point cloud to scene_pointcloud.ply")\n\n    visualize_pointcloud(points, colors)\n\nexcept ImportError:\n    print("Open3D not installed. Skipping 3D visualization.")\n\n    # Alternative: Save as simple text format\n    np.savetxt(\'pointcloud.xyz\', points, fmt=\'%.4f\')\n    print("Saved point cloud to pointcloud.xyz")\n'})}),"\n",(0,t.jsx)("checkpoint",{children:(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected"}),": 3D visualization showing the scene reconstructed from depth data."]})}),"\n",(0,t.jsx)(n.h2,{id:"expected-outcomes",children:"Expected Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"After completing this lab, you should have:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Code artifacts"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"camera_utils.py"}),": Functions for camera rendering and intrinsics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"depth_processing.py"}),": Depth image processing utilities"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"color_segmentation.py"}),": Object detection using color"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"pointcloud.py"}),": Depth to 3D conversion"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Visual outputs"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"rgb_view.png"}),": RGB camera image"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"depth_view.png"}),": Colorized depth visualization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"detection_result.png"}),": Object detection overlay"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"scene_pointcloud.ply"}),": 3D point cloud file"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Understanding"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Camera intrinsic parameters and projection"}),"\n",(0,t.jsx)(n.li,{children:"Depth image interpretation"}),"\n",(0,t.jsx)(n.li,{children:"Basic computer vision for robotics"}),"\n",(0,t.jsx)(n.li,{children:"3D reconstruction from depth"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"rubric",children:"Rubric"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Criterion"}),(0,t.jsx)(n.th,{children:"Points"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Camera rendering"}),(0,t.jsx)(n.td,{children:"15"}),(0,t.jsx)(n.td,{children:"Correctly renders RGB and depth from simulation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Intrinsics computation"}),(0,t.jsx)(n.td,{children:"15"}),(0,t.jsx)(n.td,{children:"Accurate camera matrix from model parameters"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Depth processing"}),(0,t.jsx)(n.td,{children:"20"}),(0,t.jsx)(n.td,{children:"Proper conversion to metric depth"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Color segmentation"}),(0,t.jsx)(n.td,{children:"20"}),(0,t.jsx)(n.td,{children:"Object detection with morphological cleanup"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Point cloud generation"}),(0,t.jsx)(n.td,{children:"20"}),(0,t.jsx)(n.td,{children:"Valid 3D reconstruction from depth"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Code quality"}),(0,t.jsx)(n.td,{children:"10"}),(0,t.jsx)(n.td,{children:"Well-documented, modular functions"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Total"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"100"})}),(0,t.jsx)(n.td,{})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"common-errors",children:"Common Errors"}),"\n",(0,t.jsxs)("troubleshooting",{children:[(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Error"}),": Renderer returns black image\n",(0,t.jsx)(n.strong,{children:"Solution"}),": Ensure ",(0,t.jsx)(n.code,{children:"mujoco.mj_forward()"})," is called before rendering to update the scene state."]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Error"}),": Depth values all near 1.0\n",(0,t.jsx)(n.strong,{children:"Solution"}),": Check camera position - it may be inside an object or facing away from the scene."]}),(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Error"}),": Point cloud appears stretched or distorted\n",(0,t.jsx)(n.strong,{children:"Solution"}),": Verify intrinsic matrix matches the actual rendering parameters."]})]}),"\n",(0,t.jsx)(n.h2,{id:"extensions",children:"Extensions"}),"\n",(0,t.jsx)(n.p,{children:"For advanced students:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stereo Matching"}),": Implement depth from stereo using two cameras"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Odometry"}),": Track camera motion using feature matching"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Recognition"}),": Use deep learning for object classification"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SLAM Integration"}),": Combine with IMU for simultaneous localization and mapping"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"related-content",children:"Related Content"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Theory"}),": Module 04 theory.md, Section 4.2 (Vision Systems)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Previous Lab"}),": Lab 04-01 (IMU Data Reading)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Next Lab"}),": Lab 04-03 (Sensor Fusion with Kalman Filter)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Case Study"}),": See Berkeley BAIR case study for vision-based manipulation"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}}}]);